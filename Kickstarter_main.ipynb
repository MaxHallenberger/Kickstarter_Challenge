{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kickstarer](./images/kickstarter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Imports\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import json\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RSEED = 42069"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the .csv files and concat them into one dataframe\n",
    "original_dataframe = pd.concat(map(pd.read_csv, glob.glob('data/data-2/*.csv')))\n",
    "# Reset the indices\n",
    "original_dataframe.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a working dataframe, so that we don't have to wait 10s it to import again if we want to start fresh\n",
    "df = original_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "EDA - Part 1\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only a very limited amount of suspended projects (drop), canceled projects will be treated as though they failed\n",
    "df['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate projects and store them in a table\n",
    "dups = df.groupby(df.id.tolist()).size().reset_index().rename(columns={0:'count'})\n",
    "# Sum the final col of that table, and subtract the number of culprits:\n",
    "dups['count'].sum() - dups.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Data Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features which will not be needed for further analysis\n",
    "dropped_features = ['blurb', 'currency_symbol', 'backers_count', 'is_backing', 'permissions', 'is_starred', 'source_url',\n",
    "                    'slug', 'name', 'static_usd_rate', 'profile', 'friends', 'spotlight', 'is_starrable', 'photo', 'pledged', 'usd_type',\n",
    "                    'fx_rate', 'location', 'creator', 'currency_trailing_code','current_currency', 'created_at', 'urls', 'disable_communication', 'usd_pledged' ]\n",
    "df = df.drop(dropped_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built array which contains live projects for later use \n",
    "array_live = ['live']\n",
    "live_projects = df.loc[df['state'].isin(array_live)]\n",
    "\n",
    "# Filter and concat. for target variable\n",
    "array_notlive = ['successful', 'failed', 'canceled']\n",
    "df = df.loc[df['state'].isin(array_notlive)]\n",
    "df.replace('canceled','failed', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace successful and failed entries\n",
    "df.replace(['successful','failed'],[1,0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe by 'date_changed_at' so that we will keep the entry that was most recently updated\n",
    "df.sort_values('state_changed_at')\n",
    "# Remove duplicates\n",
    "duplicates = df.duplicated(subset='id', keep='last')\n",
    "df = df[~duplicates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Feature Engineering\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['category'].apply(lambda x: json.loads(x)['slug'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['category'].apply(lambda x: x.split('/',)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new column with readable timeformat\n",
    "df['launched_at_new'] = pd.to_datetime(df['launched_at'], unit='s')\n",
    "df['deadline_new'] = pd.to_datetime(df['deadline'], unit='s')\n",
    "df['state_changed_at_new'] = pd.to_datetime(df['state_changed_at'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature 'duration' that displays wheather the project timespan was more or less than 30 days\n",
    "df = df.eval('duration = deadline - launched_at')\n",
    "df['duration'] = ['over' if x > 2592000 else 'under' for x in df['duration']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column 'time' that displays the time from project launch to project end\n",
    "df.eval('time = state_changed_at_new - launched_at_new', inplace=True)\n",
    "# Convert to days\n",
    "df['time'] = df['time'].apply(lambda x: pd.Timedelta(x).days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dates to weekend(1) or weekday(0)\n",
    "def change_time(dataframe, column_list):\n",
    "    for column in column_list:\n",
    "        dataframe[column] = [1 if x >= 6 else 0 for x in pd.to_datetime(dataframe[column], unit='s').dt.weekday]\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_lst = ['launched_at', 'deadline', 'state_changed_at']\n",
    "change_time(df, times_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "EDA - Part 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which data needs to be plotted categorical and which numerical\n",
    "categorical = ['country','currency', 'staff_pick', 'category','duration']\n",
    "numerical = ['usd_pledged', 'goal', 'converted_pledged_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot(df, column):\n",
    "    \"\"\"Generates barplots of categorical data\n",
    "\n",
    "    Args:\n",
    "        df (pd dataframe): Dataframe\n",
    "        column (object): list of names of columns which should be plotted\n",
    "    \"\"\"\n",
    "    # get feature\n",
    "    for i in column:\n",
    "        varValue = df[i].value_counts()\n",
    "\n",
    "        plt.figure(figsize = (12,3))\n",
    "        plt.bar(varValue.index, varValue, color = '#87c442', edgecolor = 'black')\n",
    "        plt.xticks(varValue.index, varValue.index.values)\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(i.capitalize())\n",
    "        plt.xticks(rotation = 90)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot(df, categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier analysis\n",
    "# We have some values in goal which are unrealistically high\n",
    "# cutoff at 100000, we are focusing on small to average kickstarter projects\n",
    "df = df.query('goal < 100000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,5))\n",
    "plt.hist(df['goal'], bins = None, facecolor = '#87c442', edgecolor = 'black');\n",
    "#range = [0.0,200000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop id (not needed anymore) and converted pledged amount\n",
    "df.drop('id', axis = 1, inplace = True)\n",
    "df.drop('converted_pledged_amount', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode all categorical data (country, currency, staff_pick, categories, disable_communication, duration) boolean values might \n",
    "# Replace True False with strings, otherwise one-hot encoding doesnt work\n",
    "df['staff_pick'].replace([True,False],['t','f'], inplace=True)\n",
    "\n",
    "\n",
    "one_hot_featurelist = ['country', 'currency', 'staff_pick', 'category', 'duration']\n",
    "one_hot = pd.get_dummies(df[one_hot_featurelist])\n",
    "df.drop(one_hot_featurelist, axis = 1, inplace=True)\n",
    "df = df.join(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalerize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_columns(df, column):\n",
    "    \"\"\"Function that scales the data with a min_max scaler\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): Dataframe\n",
    "        column (object): Name or list of names including the columns which should be normalized\n",
    "\n",
    "    Returns:\n",
    "        Dataframe object: Returns the dataframe including the normalized columns\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    for i in column:\n",
    "        scaler.fit(df[[i]])\n",
    "        df[i] = scaler.transform(df[[i]])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize numerical data\n",
    "numerical = ['goal']\n",
    "df = scale_columns(df, numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Classifier and Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features which will not be needed for further analysis\n",
    "dropped_features = ['launched_at_new','deadline_new','state_changed_at_new']\n",
    "df = df.drop(dropped_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set x and y\n",
    "x = df.drop('state', axis = 1)\n",
    "y = df['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(random_state=42069, strategy='most_frequent')"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_1 = DummyClassifier(strategy='stratified', random_state=RSEED, constant=None)\n",
    "dummy_1.fit(x,y)\n",
    "\n",
    "dummy_2 = DummyClassifier(strategy='most_frequent', random_state=RSEED, constant=None)\n",
    "dummy_2.fit(x,y)\n",
    "\n",
    "y_pred_1 = dummy_1.predict(x)\n",
    "y_pred_2 = dummy_2.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5502335947880502"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y, y_pred_1)\n",
    "f1_score(y, y_pred_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34154, 41957],\n",
       "       [41991, 51350]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y, y_pred_1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "91d869ccaeca790aa5051b91a32a203614f98fb4ba9d9ad287b8f5d4ebac826d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
